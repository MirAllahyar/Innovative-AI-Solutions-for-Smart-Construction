{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMxLAseBZ0d47I98W+eAfb5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsyyR-iO-S6V","executionInfo":{"status":"ok","timestamp":1733657236640,"user_tz":-300,"elapsed":20420,"user":{"displayName":"Ahmed Talpur","userId":"10268862620176234498"}},"outputId":"ec6bf390-541d-4a48-c694-84cd3a16525c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"]}],"source":["# Install TensorFlow\n","!pip install tensorflow\n","\n","# Install TQDM for progress display\n","!pip install tqdm\n","\n","# Install Pillow for image processing\n","!pip install pillow\n","\n","# Install Matplotlib for visualization\n","!pip install matplotlib\n","\n","!pip install numpy"]},{"cell_type":"code","source":["pip install tensorflow --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbN3zYaj-en1","executionInfo":{"status":"ok","timestamp":1733657382837,"user_tz":-300,"elapsed":70501,"user":{"displayName":"Ahmed Talpur","userId":"10268862620176234498"}},"outputId":"b60ae836-c043-4ba7-dbc3-657104459018"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Collecting tensorflow\n","  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n","Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n","  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.1\n","    Uninstalling tensorboard-2.17.1:\n","      Successfully uninstalled tensorboard-2.17.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.1\n","    Uninstalling tensorflow-2.17.1:\n","      Successfully uninstalled tensorflow-2.17.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n"]}]},{"cell_type":"code","source":["# Mount Google Drive to access the dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1rUVywVW-hHK","executionInfo":{"status":"ok","timestamp":1733657430847,"user_tz":-300,"elapsed":38785,"user":{"displayName":"Ahmed Talpur","userId":"10268862620176234498"}},"outputId":"5a6fe9bb-8178-4787-b1f9-9af97d22267a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Mount Google Drive to access the dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import necessary libraries\n","import os\n","import glob\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.optimizers import Adam\n","import json\n","\n","# Check if GPU is available\n","device_name = tf.test.gpu_device_name()\n","print(f'Using device: {device_name}' if device_name else 'GPU device not found')\n","\n","# Set the dataset path\n","dataset_path = '/content/drive/MyDrive/House_Data_2'  # Update this to your dataset location\n","\n","# Define image dimensions and parameters\n","IMG_HEIGHT = 128\n","IMG_WIDTH = 128\n","CHANNELS = 3\n","NOISE_DIM = 100\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 10000\n","EPOCHS = 2000\n","LEARNING_RATE = 1e-4\n","NUM_EXAMPLES_TO_GENERATE = 16\n","\n","# Seed for generating consistent examples\n","seed = tf.random.normal([NUM_EXAMPLES_TO_GENERATE, NOISE_DIM])\n","\n","# Function to load and preprocess images\n","def load_images(dataset_path):\n","    image_files = glob.glob(os.path.join(dataset_path, '*'))\n","    images = []\n","    for img_file in tqdm(image_files):\n","        try:\n","            img = tf.keras.preprocessing.image.load_img(img_file, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","            img = tf.keras.preprocessing.image.img_to_array(img)\n","            img = (img - 127.5) / 127.5  # Normalize images to [-1, 1]\n","            images.append(img)\n","        except Exception as e:\n","            print(f\"Error loading image {img_file}: {e}\")\n","    return np.array(images)\n","\n","# Load dataset\n","images = load_images(dataset_path)\n","print(f\"Loaded {images.shape[0]} images.\")\n","train_dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","# Generator model using U-Net with complete upsampling\n","def build_generator():\n","    inputs = tf.keras.Input(shape=(NOISE_DIM,))\n","\n","    # Fully connected layer and reshape to starting block size\n","    x = layers.Dense(8 * 8 * 512, use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU()(x)\n","    x = layers.Reshape((8, 8, 512))(x)\n","\n","    # Decoder\n","    u1 = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n","    u1 = layers.BatchNormalization()(u1)\n","    u1 = layers.LeakyReLU()(u1)\n","\n","    u2 = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(u1)\n","    u2 = layers.BatchNormalization()(u2)\n","    u2 = layers.LeakyReLU()(u2)\n","\n","    u3 = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(u2)\n","    u3 = layers.BatchNormalization()(u3)\n","    u3 = layers.LeakyReLU()(u3)\n","\n","    # Additional upsampling to reach (128, 128, 3)\n","    u4 = layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same', use_bias=False)(u3)\n","    u4 = layers.BatchNormalization()(u4)\n","    u4 = layers.LeakyReLU()(u4)\n","\n","    # Final output layer to reach (128, 128, 3)\n","    output = layers.Conv2DTranspose(CHANNELS, kernel_size=4, strides=1, padding='same', activation='tanh', use_bias=False)(u4)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=output)\n","    return model\n","\n","# Discriminator model\n","def build_discriminator():\n","    model = tf.keras.Sequential([\n","        layers.Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=[IMG_HEIGHT, IMG_WIDTH, CHANNELS]),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Conv2D(128, kernel_size=4, strides=2, padding='same'),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Conv2D(256, kernel_size=4, strides=2, padding='same'),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Flatten(),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","# Create the models\n","generator = build_generator()\n","discriminator = build_discriminator()\n","\n","# Define loss function and optimizers\n","cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","generator_optimizer = Adam(LEARNING_RATE, beta_1=0.5)\n","discriminator_optimizer = Adam(LEARNING_RATE, beta_1=0.5)\n","\n","# Training step\n","@tf.function\n","def train_step(real_images):\n","    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training=True)\n","\n","        real_output = discriminator(real_images, training=True)\n","        fake_output = discriminator(generated_images, training=True)\n","\n","        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n","        disc_loss_real = cross_entropy(tf.ones_like(real_output) * 0.9, real_output)  # Label smoothing\n","        disc_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","        disc_loss = disc_loss_real + disc_loss_fake\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","    return gen_loss, disc_loss\n","\n","# Function to generate and save images\n","def generate_and_save_images(model, epoch, test_input):\n","    predictions = model(test_input, training=False)\n","    predictions = (predictions + 1) / 2.0  # Rescale to [0, 1]\n","\n","    fig = plt.figure(figsize=(4, 4))\n","    for i in range(predictions.shape[0]):\n","        plt.subplot(4, 4, i+1)\n","        plt.imshow(predictions[i])\n","        plt.axis('off')\n","    plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n","    plt.show()\n","\n","# Save TensorFlow models and prepare for PyTorch conversion\n","def save_model_as_pt(generator, discriminator):\n","    # Save TensorFlow/Keras models\n","    generator_json = generator.to_json()\n","    with open(\"generator_model.json\", \"w\") as json_file:\n","        json.dump(generator_json, json_file)\n","    generator.save_weights(\"generator_weights.h5\")\n","\n","    discriminator_json = discriminator.to_json()\n","    with open(\"discriminator_model.json\", \"w\") as json_file:\n","        json.dump(discriminator_json, json_file)\n","    discriminator.save_weights(\"discriminator_weights.h5\")\n","\n","    print(\"TensorFlow/Keras models saved. Ready for PyTorch conversion.\")\n","\n","# Training loop\n","def train(dataset, epochs):\n","    for epoch in range(epochs):\n","        print(f\"Epoch {epoch + 1}/{epochs}\")\n","        for image_batch in dataset:\n","            gen_loss, disc_loss = train_step(image_batch)\n","\n","        if (epoch + 1) % 100 == 0:\n","            generate_and_save_images(generator, epoch + 1, seed)\n","            print(f\"Epoch {epoch + 1}, Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}\")\n","\n","        if (epoch + 1) % 500 == 0:\n","            generator.save_weights(f'generator_epoch_{epoch + 1}.weights.h5')\n","            discriminator.save_weights(f'discriminator_epoch_{epoch + 1}.weights.h5')\n","\n","    generate_and_save_images(generator, epochs, seed)\n","    save_model_as_pt(generator, discriminator)\n","    print(\"Model training completed. Models saved.\")\n","\n","# Start training\n","train(train_dataset, EPOCHS)\n","# Mount Google Drive to access the dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import necessary libraries\n","import os\n","import glob\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.optimizers import Adam\n","import json\n","\n","# Check if GPU is available\n","device_name = tf.test.gpu_device_name()\n","print(f'Using device: {device_name}' if device_name else 'GPU device not found')\n","\n","# Set the dataset path\n","dataset_path = '/content/drive/MyDrive/House_Data_2'  # Update this to your dataset location\n","\n","# Define image dimensions and parameters\n","IMG_HEIGHT = 128\n","IMG_WIDTH = 128\n","CHANNELS = 3\n","NOISE_DIM = 100\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 10000\n","EPOCHS = 2000\n","LEARNING_RATE = 1e-4\n","NUM_EXAMPLES_TO_GENERATE = 16\n","\n","# Seed for generating consistent examples\n","seed = tf.random.normal([NUM_EXAMPLES_TO_GENERATE, NOISE_DIM])\n","\n","# Function to load and preprocess images\n","def load_images(dataset_path):\n","    image_files = glob.glob(os.path.join(dataset_path, '*'))\n","    images = []\n","    for img_file in tqdm(image_files):\n","        try:\n","            img = tf.keras.preprocessing.image.load_img(img_file, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","            img = tf.keras.preprocessing.image.img_to_array(img)\n","            img = (img - 127.5) / 127.5  # Normalize images to [-1, 1]\n","            images.append(img)\n","        except Exception as e:\n","            print(f\"Error loading image {img_file}: {e}\")\n","    return np.array(images)\n","\n","# Load dataset\n","images = load_images(dataset_path)\n","print(f\"Loaded {images.shape[0]} images.\")\n","train_dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","# Generator model using U-Net with complete upsampling\n","def build_generator():\n","    inputs = tf.keras.Input(shape=(NOISE_DIM,))\n","\n","    # Fully connected layer and reshape to starting block size\n","    x = layers.Dense(8 * 8 * 512, use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU()(x)\n","    x = layers.Reshape((8, 8, 512))(x)\n","\n","    # Decoder\n","    u1 = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n","    u1 = layers.BatchNormalization()(u1)\n","    u1 = layers.LeakyReLU()(u1)\n","\n","    u2 = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(u1)\n","    u2 = layers.BatchNormalization()(u2)\n","    u2 = layers.LeakyReLU()(u2)\n","\n","    u3 = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(u2)\n","    u3 = layers.BatchNormalization()(u3)\n","    u3 = layers.LeakyReLU()(u3)\n","\n","    # Additional upsampling to reach (128, 128, 3)\n","    u4 = layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same', use_bias=False)(u3)\n","    u4 = layers.BatchNormalization()(u4)\n","    u4 = layers.LeakyReLU()(u4)\n","\n","    # Final output layer to reach (128, 128, 3)\n","    output = layers.Conv2DTranspose(CHANNELS, kernel_size=4, strides=1, padding='same', activation='tanh', use_bias=False)(u4)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=output)\n","    return model\n","\n","# Discriminator model\n","def build_discriminator():\n","    model = tf.keras.Sequential([\n","        layers.Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=[IMG_HEIGHT, IMG_WIDTH, CHANNELS]),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Conv2D(128, kernel_size=4, strides=2, padding='same'),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Conv2D(256, kernel_size=4, strides=2, padding='same'),\n","        layers.LeakyReLU(),\n","        layers.Dropout(0.3),\n","        layers.Flatten(),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","# Create the models\n","generator = build_generator()\n","discriminator = build_discriminator()\n","\n","# Define loss function and optimizers\n","cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","generator_optimizer = Adam(LEARNING_RATE, beta_1=0.5)\n","discriminator_optimizer = Adam(LEARNING_RATE, beta_1=0.5)\n","\n","# Training step\n","@tf.function\n","def train_step(real_images):\n","    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training=True)\n","\n","        real_output = discriminator(real_images, training=True)\n","        fake_output = discriminator(generated_images, training=True)\n","\n","        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n","        disc_loss_real = cross_entropy(tf.ones_like(real_output) * 0.9, real_output)  # Label smoothing\n","        disc_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","        disc_loss = disc_loss_real + disc_loss_fake\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","    return gen_loss, disc_loss\n","\n","# Function to generate and save images\n","def generate_and_save_images(model, epoch, test_input):\n","    predictions = model(test_input, training=False)\n","    predictions = (predictions + 1) / 2.0  # Rescale to [0, 1]\n","\n","    fig = plt.figure(figsize=(4, 4))\n","    for i in range(predictions.shape[0]):\n","        plt.subplot(4, 4, i+1)\n","        plt.imshow(predictions[i])\n","        plt.axis('off')\n","    plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n","    plt.show()\n","\n","# Save TensorFlow models and prepare for PyTorch conversion\n","def save_model_as_pt(generator, discriminator):\n","    # Save TensorFlow/Keras models\n","    generator_json = generator.to_json()\n","    with open(\"generator_model.json\", \"w\") as json_file:\n","        json.dump(generator_json, json_file)\n","    generator.save_weights(\"generator_weights.h5\")\n","\n","    discriminator_json = discriminator.to_json()\n","    with open(\"discriminator_model.json\", \"w\") as json_file:\n","        json.dump(discriminator_json, json_file)\n","    discriminator.save_weights(\"discriminator_weights.h5\")\n","\n","    print(\"TensorFlow/Keras models saved. Ready for PyTorch conversion.\")\n","\n","# Training loop\n","def train(dataset, epochs):\n","    for epoch in range(epochs):\n","        print(f\"Epoch {epoch + 1}/{epochs}\")\n","        for image_batch in dataset:\n","            gen_loss, disc_loss = train_step(image_batch)\n","\n","        if (epoch + 1) % 100 == 0:\n","            generate_and_save_images(generator, epoch + 1, seed)\n","            print(f\"Epoch {epoch + 1}, Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}\")\n","\n","        if (epoch + 1) % 500 == 0:\n","            generator.save_weights(f'generator_epoch_{epoch + 1}.weights.h5')\n","            discriminator.save_weights(f'discriminator_epoch_{epoch + 1}.weights.h5')\n","\n","    generate_and_save_images(generator, epochs, seed)\n","    save_model_as_pt(generator, discriminator)\n","    print(\"Model training completed. Models saved.\")\n","\n","# Start training\n","train(train_dataset,Epoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"o90PnCwp-jxO","executionInfo":{"status":"error","timestamp":1733657660882,"user_tz":-300,"elapsed":900,"user":{"displayName":"Ahmed Talpur","userId":"10268862620176234498"}},"outputId":"5bf4e027-f99d-40f9-9142-309515f1ec02"},"execution_count":12,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid non-printable character U+00A0 (<ipython-input-12-c6edea488ef8>, line 188)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-c6edea488ef8>\"\u001b[0;36m, line \u001b[0;32m188\u001b[0m\n\u001b[0;31m    train(train_dataset, EPOCHS)\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"]}]}]}